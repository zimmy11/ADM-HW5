{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 5 - USA Airport Flight Analysis**\n",
    "\n",
    "*Group#12*\n",
    "\n",
    "- **Marco Zimmatore** - [zimmatore.1947442@studenti.uniroma1.it](mailto:zimmatore.1947442@studenti.uniroma1.it)\n",
    "- **Davide Vitale** - [vitale.1794386@studenti.uniroma1.it](mailto:vitale.1794386@studenti.uniroma1.it)\n",
    "- **Darkhan Maksutov** - [maksutov.2113209@studenti.uniroma1.it](mailto:maksutov.2113209@studenti.uniroma1.it)\n",
    "- **Riccardo Soleo** - [soleo.1911063@studenti.uniroma1.it](mailto:soleo.1911063@studenti.uniroma1.it)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from IPython.display import display, Markdown\n",
    "import warnings\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "dataset_path = \"./data/Airports2.csv\"       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dataset_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We wanna build a graph with edges for each route, where each node represents an airport and eac edge is the existent route between two airports, with all the infos on that route. For this reason, we decide to remove all the rows with *NA* values in them, because each information is crucial for the next exercises computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We remove also all the flights that have $Distance = 0$, because they dont make sense for our next exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df = df[df['Distance'] > 0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We print the data types of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Before storing the dataframe into a Graph, we wanna decide if the graph has to be directed or undirected. For this reason we compute the number of *Bidirectional* routes compared to the number of *Unidirectional* routes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe\n",
    "df_undirected_edges = df[['Origin_airport', 'Destination_airport']].copy()\n",
    "\n",
    "# Create a normalized key to represent bidirectional routes\n",
    "df_undirected_edges['normalized_key'] = df.apply(\n",
    "    lambda row: tuple(sorted([row['Origin_airport'], row['Destination_airport']])), axis=1\n",
    ")\n",
    "\n",
    "# Add a column to represent the actual direction of the route\n",
    "df_undirected_edges['direction'] = df.apply(\n",
    "    lambda row: (row['Origin_airport'], row['Destination_airport']), axis=1\n",
    ")\n",
    "\n",
    "# Add a column to represent the reverse direction of the route\n",
    "df_undirected_edges['reverse_direction'] = df.apply(\n",
    "    lambda row: (row['Destination_airport'], row['Origin_airport']), axis=1\n",
    ")\n",
    "\n",
    "# Group by normalized_key and collect all directions and reverse directions\n",
    "grouped_undirected = df_undirected_edges.groupby('normalized_key').agg(\n",
    "    directions=('direction', list),  # List of all actual directions\n",
    "    reverse_directions=('reverse_direction', list)  # List of all reverse directions\n",
    ").reset_index()\n",
    "\n",
    "# Check if both directions exist for each route\n",
    "grouped_undirected['is_bidirectional'] = grouped_undirected.apply(\n",
    "    lambda row: 1 if any(d in row['reverse_directions'] for d in row['directions']) else 0, axis=1\n",
    ")\n",
    "# We compute the Percentage between bidirectional and unidirectiona routes\n",
    "grouped_undirected['is_bidirectional'].value_counts() * 100 / grouped_undirected.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Since the Number of Bidirectional routes is not too high compared to the Unidirectional Routes, we can proceed using a **Directed Graph** module in *NetworkX*.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Flight Network Analysis (Q1)**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going deep in the exercises, we store the Dataframe in a Graph.\n",
    "\n",
    "**Function** `create_airport_graph`\n",
    "\n",
    "- The function `create_airport_graph(df)` creates a directed graph using the **Networkx** library from a DataFrame containing airports and flight data.\n",
    "\n",
    "- For the reasons explained above, we decide to use a **DiGraph** structure because we want to store only 2 connections per route at max. \n",
    "\n",
    "- For each row in the DataFrame, it adds two nodes (representing the origin and destination airports) to the graph, along with attributes such as city, population, latitude, and longitude.\n",
    "\n",
    "-  It then adds a directed edge between the origin and destination airports, with attributes like the number of passengers, flights, seats, **weight as distance**, and flight date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = functions.create_airport_graph(df)\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can notice that only a subset of all the connections have been stored, since the *NetworkX* *DiGraph* structure allows only two edges per route at max."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: all the functions displayed and explained below in the $Q_1$, will be runned in the last section (to generate the report), in order to be tidier and not to be too verbose, so that the report can be understood without ripetitions.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implement a function `analyze_graph_features(flight_network)` that takes the flight network as input and computes the following:\n",
    "\n",
    "    - Count the number of airports (`nodes`) and flights (`edges`) in the graph.\n",
    "\n",
    "    - Compute the density of the graph using the formula: $ Density = \\frac{2\\times E}{N(N − 1)}$\n",
    "\n",
    "    - Calculate both `in-degree` and `out-degree` for each airport and visualize them using histograms.\n",
    "\n",
    "    - Identify airports with degrees higher than the 90th percentile and list them as \"`hubs`\".\n",
    "    \n",
    "    - Determine if the graph is sparse or dense based on its density.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function** `analyze_graph_features(flight_network)` \n",
    "\n",
    "- It counts the total number of nodes (airports) and edges (flight routes) in the graph iterating over the number of the nodes and the edges to calculate the *in-degrees* and *out_degrees* of each node and storing them into a dictionary.\n",
    "\n",
    "- It calculates the graph density, which is the ratio of the actual number of edges to the maximum possible number of edges in a directed graph with the same number of nodes. Since we are using a **Directed Graph** the formula used is: $$Density = \\frac{\\text{Number of Edges}}{\\text{Number of nodes} \\times (\\text{Number of nodes} - 1)}$$ \n",
    "\n",
    "- Then we create two Histograms using **Plotly**, one for *in-degrees* and one for *out-degrees*\n",
    "\n",
    "- It calculates the 90th percentile of the total degree (sum of in-degree and out-degree) across all nodes. Nodes that have a degree higher than this percentile are considered \"**hubs**\" (highly connected airports).\n",
    "\n",
    "- In the end, it checks whether the graph is dense or sparse based on the calculated graph density. If the density is greater than a threshold (0.5), the graph is considered dense, otherwise it is sparse.\n",
    "​\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_graph_features(flight_network):\n",
    "    # Initialize variables to store the number of nodes and edges\n",
    "    number_of_nodes = 0\n",
    "    number_of_edges = len(flight_network.edges())  # Count the number of edges in the graph\n",
    "    dict_degrees_edges = dict()  # Create an empty dictionary to store in-degrees and out-degrees for each node\n",
    "\n",
    "    # Iterate over each node in the flight network\n",
    "    for node in flight_network.nodes:\n",
    "        number_of_nodes = number_of_nodes + 1  # Increment the node count\n",
    "\n",
    "        in_edges = 0  # Initialize in-degree counter\n",
    "        out_edges = 0  # Initialize out-degree counter\n",
    "\n",
    "        # Count the outgoing edges from the current node\n",
    "        for _, _, attr in flight_network.edges(node, data=True):\n",
    "            out_edges += 1\n",
    "\n",
    "        # Count the incoming edges to the current node\n",
    "        for _, _, attr in flight_network.in_edges(node, data=True):\n",
    "            in_edges += 1\n",
    "\n",
    "        # Store the in-degree and out-degree for the current node in the dictionary\n",
    "        dict_degrees_edges[node] = [in_edges, out_edges]\n",
    "\n",
    "    # Calculate graph density using the formula\n",
    "    graph_density = (number_of_edges) / (number_of_nodes * (number_of_nodes - 1))\n",
    "\n",
    "    # Extract the in-degrees and out-degrees from the dictionary for histogram plotting\n",
    "    in_degrees = [edge_degree[0] for edge_degree in dict_degrees_edges.values()]\n",
    "    out_degrees = [edge_degree[1] for edge_degree in dict_degrees_edges.values()]\n",
    "\n",
    "    # Create a subplot with 1 row and 2 columns to display histograms for in-degrees and out-degrees\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=('In-degree Histogram', 'Out-degree Histogram'))\n",
    "\n",
    "    # Add in-degree histogram to the first subplot (left)\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=in_degrees, nbinsx=20, name='In-degree', marker=dict(color='steelblue')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Add out-degree histogram to the second subplot (right)\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=out_degrees, nbinsx=20, name='Out-degree', marker=dict(color='darkorange')),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    # Update layout for better aesthetics\n",
    "    fig.update_layout(\n",
    "        title=\"In-degree vs Out-degree Histograms\",  # Set the title of the plot\n",
    "        xaxis_title=\"Degree\",  # Label for x-axis\n",
    "        yaxis_title=\"Frequency\",  # Label for y-axis\n",
    "        showlegend=True,  # Display legend\n",
    "        height=500,  # Adjust the height of the figure\n",
    "        width=1000   # Adjust the width of the figure\n",
    "    )\n",
    "\n",
    "    # Calculate the 90th percentile for the total degrees (in-degree + out-degree) of each node\n",
    "\n",
    "    # Build a dictionary to compute the total degree value (in-degree + out-degree) for each node\n",
    "    dict_degrees = dict()\n",
    "\n",
    "    # Iterate over the nodes and calculate the total degree for each\n",
    "    for node, degrees in dict_degrees_edges.items():\n",
    "        dict_degrees[node] = degrees[0] + degrees[1]  # Sum in-degree and out-degree for each node\n",
    "\n",
    "    # Use numpy's percentile function to get the 90th percentile of the degrees\n",
    "    degree_percentile = np.percentile(list(dict_degrees.values()), 90)\n",
    "\n",
    "    # Identify nodes (airports) that are \"hubs\", meaning their total degree is greater than the 90th percentile\n",
    "    hubs = []\n",
    "\n",
    "    # Iterate over the nodes and check if their total degree exceeds the 90th percentile\n",
    "    for node, degree in dict_degrees.items():\n",
    "        if degree > degree_percentile:\n",
    "            hubs.append((node, degree))  # Add the node and its degree to the list of hubs\n",
    "\n",
    "    threshold = 0.5  # Set a threshold to decide if the graph is dense or sparse\n",
    "\n",
    "    # Check if the graph is dense or sparse based on the density calculated earlier\n",
    "    if graph_density > threshold:\n",
    "        is_sparse = False  # If the density is greater than the threshold, the graph is considered dense\n",
    "    else:\n",
    "        is_sparse = True  # Otherwise, the graph is considered sparse\n",
    "\n",
    "    # Return the number of nodes, the number of edges, the figure with histograms, the list of hubs, and whether the graph is sparse\n",
    "    return number_of_nodes, number_of_edges, fig, hubs, is_sparse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a function `summarize_graph_features(flight_network)` that generates a detailed report of the graph's features. A summary report needs to include:\n",
    "\n",
    "    - The number of nodes and edges.\n",
    "    \n",
    "    - The graph density.\n",
    "    \n",
    "    - Degree distribution plots for in-degree and out-degree.\n",
    "    \n",
    "    - A table of identified hubs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function** `summarize_graph_features(flight_network)`\n",
    "\n",
    "\n",
    "The `summarize_graph_features(flight_network)` function analyzes the features of a flight network graph and generates a summary of the graph's properties:\n",
    "\n",
    "- **Analyzes Graph Features**: It uses the `analyze_graph_features` function to compute key metrics of the flight network, we have described above\n",
    "\n",
    "- **Creates a Summary Table**: The function generates a markdown table summarizing the following metrics:\n",
    "\n",
    "    - **Number of airports**\n",
    "\n",
    "    - **Number of flights** \n",
    "\n",
    "    - **Graph density**\n",
    "\n",
    "    - **Graph classification** (whether the graph is sparse or dense)\n",
    "\n",
    "- **Displays Hubs**: It creates and displays a table of identified \"*hubs*\" listing the airports with the highest degree in the network, alongside their degree values (number of connections).\n",
    "\n",
    "- **Displays Degree Distribution**: The function also visualizes and displays the degree distribution histogram, which shows the distribution of in-degrees and out-degrees across the airports (nodes) in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_graph_features(flight_network):\n",
    "    # Analyze graph features\n",
    "    number_of_nodes, number_of_edges, degree_histogram, hubs, is_sparse = analyze_graph_features(flight_network)\n",
    "\n",
    "    # Create a textual summary\n",
    "    density_description = \"dense\" if not is_sparse else \"sparse\"\n",
    "    summary_table = f\"\"\"\n",
    "| Metric                  | Value                      |\n",
    "|-------------------------|----------------------------|\n",
    "| **Number of Airports**      | {number_of_nodes}          |\n",
    "| **Number of Flights**       | {number_of_edges}          |\n",
    "| **Graph Density**           | {'{:.4f}'.format((2 * number_of_edges) / (number_of_nodes * (number_of_nodes - 1)))}|\n",
    "| **Graph Classification**    | {density_description.capitalize()} |\n",
    "\"\"\"\n",
    "\n",
    "    row_labels = \"| Hubs (Airports)          | \" + \" | \".join([hub[0] for hub in hubs]) + \" |\\n\"\n",
    "    separator_row = \"|-----------------| \" + \" | \".join([\"---\"] * len(hubs)) + \" |\\n\"\n",
    "    # Create the degree row\n",
    "    degree_row = \"| **Degrees**          | \" + \" | \".join([str(hub[1]) for hub in hubs]) + \" |\\n\"\n",
    "\n",
    "    # Combine rows into the Markdown table\n",
    "    hubs_table = row_labels + separator_row + degree_row\n",
    "\n",
    "    display(Markdown(\"## **Graph Features Summary**\"))\n",
    "\n",
    "    # Display summary\n",
    "    display(Markdown(summary_table))\n",
    "\n",
    "    display(Markdown(\"### **Identified Hubs**\"))\n",
    "    # Display the hubs table\n",
    "    display(Markdown(hubs_table))\n",
    "\n",
    "    # Display the degree distribution histogram\n",
    "    display(Markdown(\"### **Degree Distribution**\"))\n",
    "    degree_histogram.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now let's dive deeper into the analysis of the dataset. Do the following:\n",
    "    \n",
    "    - Compute total passenger flow between origin and destination cities.\n",
    "\n",
    "    - Identify and visualize the busiest routes by passenger traffic.\n",
    "\n",
    "    - Calculate the average passengers per flight for each route and highlight under/over-utilized connections.\n",
    "\n",
    "    - Create an interactive map visualizing the geographic spread of the flight network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We split the workload of this exercise in two functions: `analysis_traffic_passengers(df)` and  `create_interactive_map(df)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function**  `analysis_traffic_passengers(df)`\n",
    "\n",
    "The `analysis_traffic_passengers` function analyzes passenger traffic data for flight routes and generates several outputs to help understand the busiest routes, average passengers per route, and least traveled routes:\n",
    "\n",
    "-  The function takes in a *DataFrame* containing flight data and an optional parameter, `number_of_busiest_routes` (default set to 10), to define how many routes to display. \n",
    "\n",
    "- **Busiest Routes** (by Total Passengers):\n",
    "\n",
    "    - The data is grouped by origin and destination airports, and the total number of passengers per route is calculated.\n",
    "\n",
    "    - The routes are sorted by total passengers in descending order, and the top `number_of_busiest_routes` routes are selected.\n",
    "    \n",
    "    - A new column, `Route`, is created by combining the origin and destination airports for easy display.\n",
    "\n",
    "    - A Plotly bar plot is generated to visualize the busiest routes with the total number of passengers.\n",
    "\n",
    "- **Average Traffic Per Route**:\n",
    "\n",
    "    - The function calculates the average number of passengers per route by grouping the data by origin and destination airports.\n",
    "\n",
    "    - The routes are sorted by average passengers in descending order to identify the busiest routes in terms of average passengers.\n",
    "\n",
    "    - Similarly, the routes with the least average passengers are sorted in ascending order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted_passengers, fig, df_most_traffic, df_least_traffic = functions.analysis_traffic_passengers(df) \n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can notice that most of the busiest routes connect the major American cities to *Honolulu* (Hawaii) underlining that in the last 30 years, Hawaii has been the most loved place to go on vacation by American citizens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function** `create_interactive_map(df)`\n",
    "\n",
    "The `create_interactive_map` function generates an interactive map using *Folium* to visualize flight routes and airports. \n",
    "\n",
    "- It initializes a map centered on the US.\n",
    "\n",
    "- Filters the dataset to select only the Busiest flights based on the number of passenger, because we want to draw only one connection per route's direction. So birectional routes will have two *PolyLines* that connect he airports.\n",
    "\n",
    "- It adds flight conections as *Folium* **PolyLines** (lines connecting origin and destination airports).\n",
    "\n",
    "- Adds *Folium* markers for each airport, displaying airport names and cities when clicked.\n",
    "\n",
    "- Saves the map as an HTML file for interactive exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions.create_interactive_map(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Once you have created and tested the previous functions, the results should be presented in a tidy way. Your summary report should contain:\n",
    "\n",
    "- The number of nodes and edges.\n",
    "\n",
    "-    The graph density.\n",
    "\n",
    "-    Degree distribution plots for in-degree and out-degree.\n",
    "\n",
    "-    A table of identified hubs.\n",
    "\n",
    "-    Top routes by passenger flow (table and bar chart).\n",
    "\n",
    "-    Top routes by passenger efficiency (table and bar chart).\n",
    "\n",
    "-    An interactive map showing flight routes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function** `generate_report(df, graph)`\n",
    "\n",
    "The `generate_report` function generates a detailed report on the flight network, including the following:\n",
    "\n",
    "- **Graph Features Summary**: Summarizes the features of the flight network calling the function `summarize_graph_features`.\n",
    "\n",
    "- **Traffic Analysis**: Displays the busiest routes by passenger flow, the most and least trafficked routes calling the function `analysis_traffic_passengers`\n",
    "\n",
    "- **Under-Utilized and Over-Utilized Routes**: Shows routes with the least and most passenger traffic.\n",
    "\n",
    "- **Top Routes by Passenger Efficiency**:\n",
    "\n",
    "    - It calculates and displays the most efficient routes (Top 10) with a corresponding bar chart. In our interpretation, *Passenger Efficiency* is the number of Passengers transported per Km for each route.\n",
    "\n",
    "    - We have computed the **Passenger Efficiency** as \n",
    "$$ \\text{Passenger Efficiency} = \\frac{\\text{Average Number of Passengers of that Route}}{\\text{Route Distance}}$$\n",
    "\n",
    "\n",
    "**Remark**: we have included the interactive map in the *flight_network_map.html* file saved by the function `create_interactive_map` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions.generate_report(df, graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "After completing the analysis, answer the following questions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Is the graph sparse or dense?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Since the density of the graph is quite near 0, we can assume that the graph is **sparse**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  What patterns do you observe in the degree distribution?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Both Histograms of *out_degree* and *in_degree* of each edge assume similar distributions: most of the nodes tend to be have 20 or less number of edges, another slice of nodes have in and out degrees between 20 and 100.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which airports are identified as hubs, and why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Hubs (Airports)          | SEA | SFO | LAX | FLL | PHX | TUS | DFW | SLC | LAS | ICT | OKC | IAH | ELP | TUL | OMA | RFD | MKE | LIT | SHV | MCI | SAT | MSP | ORD | STL | BNA | MEM | IND | CLE | DTW | DAY | CVG | CMH | PIT | BOS | ATL | MDW | PHL | EWR | CLT | JFK | YIP | MCO | IAD | MSY | RDU | BWI | TYS | MIA | DAL |\n",
    "|-----------------| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| **Degrees**          | 392 | 368 | 437 | 376 | 476 | 417 | 496 | 459 | 463 | 386 | 455 | 439 | 391 | 367 | 383 | 383 | 453 | 404 | 403 | 498 | 384 | 568 | 514 | 501 | 430 | 525 | 498 | 429 | 457 | 375 | 438 | 397 | 435 | 387 | 511 | 408 | 423 | 424 | 412 | 406 | 543 | 411 | 469 | 387 | 409 | 441 | 384 | 481 | 384 |\n",
    "\n",
    "<br>\n",
    "\n",
    "> These airports are listed as '*hubs*' because their degree is higher than 90% of the degree of all the nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What are the busiest routes in terms of passenger traffic?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "</style>\n",
    "<table id=\"T_c0f38\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th id=\"T_c0f38_level0_col0\" class=\"col_heading level0 col0\" >Origin_airport</th>\n",
    "      <th id=\"T_c0f38_level0_col1\" class=\"col_heading level0 col1\" >Destination_airport</th>\n",
    "      <th id=\"T_c0f38_level0_col2\" class=\"col_heading level0 col2\" >Total_Passengers</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td id=\"T_c0f38_row0_col0\" class=\"data row0 col0\" >OGG</td>\n",
    "      <td id=\"T_c0f38_row0_col1\" class=\"data row0 col1\" >HNL</td>\n",
    "      <td id=\"T_c0f38_row0_col2\" class=\"data row0 col2\" >32364612</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td id=\"T_c0f38_row1_col0\" class=\"data row1 col0\" >HNL</td>\n",
    "      <td id=\"T_c0f38_row1_col1\" class=\"data row1 col1\" >OGG</td>\n",
    "      <td id=\"T_c0f38_row1_col2\" class=\"data row1 col2\" >29744742</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td id=\"T_c0f38_row2_col0\" class=\"data row2 col0\" >LAX</td>\n",
    "      <td id=\"T_c0f38_row2_col1\" class=\"data row2 col1\" >HNL</td>\n",
    "      <td id=\"T_c0f38_row2_col2\" class=\"data row2 col2\" >28964154</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td id=\"T_c0f38_row3_col0\" class=\"data row3 col0\" >HNL</td>\n",
    "      <td id=\"T_c0f38_row3_col1\" class=\"data row3 col1\" >LAX</td>\n",
    "      <td id=\"T_c0f38_row3_col2\" class=\"data row3 col2\" >28632161</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td id=\"T_c0f38_row4_col0\" class=\"data row4 col0\" >LAS</td>\n",
    "      <td id=\"T_c0f38_row4_col1\" class=\"data row4 col1\" >LAX</td>\n",
    "      <td id=\"T_c0f38_row4_col2\" class=\"data row4 col2\" >26333721</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td id=\"T_c0f38_row5_col0\" class=\"data row5 col0\" >LAX</td>\n",
    "      <td id=\"T_c0f38_row5_col1\" class=\"data row5 col1\" >LAS</td>\n",
    "      <td id=\"T_c0f38_row5_col2\" class=\"data row5 col2\" >26177809</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td id=\"T_c0f38_row6_col0\" class=\"data row6 col0\" >LAX</td>\n",
    "      <td id=\"T_c0f38_row6_col1\" class=\"data row6 col1\" >SFO</td>\n",
    "      <td id=\"T_c0f38_row6_col2\" class=\"data row6 col2\" >25661782</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td id=\"T_c0f38_row7_col0\" class=\"data row7 col0\" >SFO</td>\n",
    "      <td id=\"T_c0f38_row7_col1\" class=\"data row7 col1\" >LAX</td>\n",
    "      <td id=\"T_c0f38_row7_col2\" class=\"data row7 col2\" >25458207</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td id=\"T_c0f38_row8_col0\" class=\"data row8 col0\" >ATL</td>\n",
    "      <td id=\"T_c0f38_row8_col1\" class=\"data row8 col1\" >MCO</td>\n",
    "      <td id=\"T_c0f38_row8_col2\" class=\"data row8 col2\" >23483751</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td id=\"T_c0f38_row9_col0\" class=\"data row9 col0\" >ORD</td>\n",
    "      <td id=\"T_c0f38_row9_col1\" class=\"data row9 col1\" >LAX</td>\n",
    "      <td id=\"T_c0f38_row9_col2\" class=\"data row9 col2\" >22979359</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "> These routes represent the 10 highest traffic connections, highlighting the demand for travel between major airports, especially in regions like California and Hawaii.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which routes are under/over-utilized?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Under-Utilized Routes**\n",
    "\n",
    "<style type=\"text/css\">\n",
    "</style>\n",
    "<table id=\"T_c15c7\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th id=\"T_c15c7_level0_col0\" class=\"col_heading level0 col0\" >Origin_airport</th>\n",
    "      <th id=\"T_c15c7_level0_col1\" class=\"col_heading level0 col1\" >Destination_airport</th>\n",
    "      <th id=\"T_c15c7_level0_col2\" class=\"col_heading level0 col2\" >Average_Passengers</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td id=\"T_c15c7_row0_col0\" class=\"data row0 col0\" >ABE</td>\n",
    "      <td id=\"T_c15c7_row0_col1\" class=\"data row0 col1\" >ACT</td>\n",
    "      <td id=\"T_c15c7_row0_col2\" class=\"data row0 col2\" >0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td id=\"T_c15c7_row1_col0\" class=\"data row1 col0\" >IDA</td>\n",
    "      <td id=\"T_c15c7_row1_col1\" class=\"data row1 col1\" >SGU</td>\n",
    "      <td id=\"T_c15c7_row1_col2\" class=\"data row1 col2\" >0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td id=\"T_c15c7_row2_col0\" class=\"data row2 col0\" >ILG</td>\n",
    "      <td id=\"T_c15c7_row2_col1\" class=\"data row2 col1\" >ABE</td>\n",
    "      <td id=\"T_c15c7_row2_col2\" class=\"data row2 col2\" >0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td id=\"T_c15c7_row3_col0\" class=\"data row3 col0\" >ILG</td>\n",
    "      <td id=\"T_c15c7_row3_col1\" class=\"data row3 col1\" >ADS</td>\n",
    "      <td id=\"T_c15c7_row3_col2\" class=\"data row3 col2\" >0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td id=\"T_c15c7_row4_col0\" class=\"data row4 col0\" >ILG</td>\n",
    "      <td id=\"T_c15c7_row4_col1\" class=\"data row4 col1\" >BDL</td>\n",
    "      <td id=\"T_c15c7_row4_col2\" class=\"data row4 col2\" >0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td id=\"T_c15c7_row5_col0\" class=\"data row5 col0\" >ILG</td>\n",
    "      <td id=\"T_c15c7_row5_col1\" class=\"data row5 col1\" >BIF</td>\n",
    "      <td id=\"T_c15c7_row5_col2\" class=\"data row5 col2\" >0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td id=\"T_c15c7_row6_col0\" class=\"data row6 col0\" >ILG</td>\n",
    "      <td id=\"T_c15c7_row6_col1\" class=\"data row6 col1\" >BNA</td>\n",
    "      <td id=\"T_c15c7_row6_col2\" class=\"data row6 col2\" >0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td id=\"T_c15c7_row7_col0\" class=\"data row7 col0\" >ILG</td>\n",
    "      <td id=\"T_c15c7_row7_col1\" class=\"data row7 col1\" >BRO</td>\n",
    "      <td id=\"T_c15c7_row7_col2\" class=\"data row7 col2\" >0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td id=\"T_c15c7_row8_col0\" class=\"data row8 col0\" >ILG</td>\n",
    "      <td id=\"T_c15c7_row8_col1\" class=\"data row8 col1\" >BUF</td>\n",
    "      <td id=\"T_c15c7_row8_col2\" class=\"data row8 col2\" >0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td id=\"T_c15c7_row9_col0\" class=\"data row9 col0\" >ILG</td>\n",
    "      <td id=\"T_c15c7_row9_col1\" class=\"data row9 col1\" >CAK</td>\n",
    "      <td id=\"T_c15c7_row9_col2\" class=\"data row9 col2\" >0.0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "<br>\n",
    "\n",
    "> These routes are under-utilized considering the assumption that we have not considered routes that have no *Distance*, with no passengers recorded on average. This may indicate low demand or possibly infrequent flights, which could suggest an opportunity for route optimization or better marketing strategies to increase passenger traffic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Over-Utilized Routes**\n",
    "\n",
    "<style type=\"text/css\">\n",
    "</style>\n",
    "<table id=\"T_fc661\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th id=\"T_fc661_level0_col0\" class=\"col_heading level0 col0\" >Origin_airport</th>\n",
    "      <th id=\"T_fc661_level0_col1\" class=\"col_heading level0 col1\" >Destination_airport</th>\n",
    "      <th id=\"T_fc661_level0_col2\" class=\"col_heading level0 col2\" >Average_Passengers</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td id=\"T_fc661_row0_col0\" class=\"data row0 col0\" >DAL</td>\n",
    "      <td id=\"T_fc661_row0_col1\" class=\"data row0 col1\" >HOU</td>\n",
    "      <td id=\"T_fc661_row0_col2\" class=\"data row0 col2\" >21828.4</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td id=\"T_fc661_row1_col0\" class=\"data row1 col0\" >HOU</td>\n",
    "      <td id=\"T_fc661_row1_col1\" class=\"data row1 col1\" >DAL</td>\n",
    "      <td id=\"T_fc661_row1_col2\" class=\"data row1 col2\" >21686.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td id=\"T_fc661_row2_col0\" class=\"data row2 col0\" >LGA</td>\n",
    "      <td id=\"T_fc661_row2_col1\" class=\"data row2 col1\" >DCA</td>\n",
    "      <td id=\"T_fc661_row2_col2\" class=\"data row2 col2\" >15371.6</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td id=\"T_fc661_row3_col0\" class=\"data row3 col0\" >DCA</td>\n",
    "      <td id=\"T_fc661_row3_col1\" class=\"data row3 col1\" >LGA</td>\n",
    "      <td id=\"T_fc661_row3_col2\" class=\"data row3 col2\" >14628.6</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td id=\"T_fc661_row4_col0\" class=\"data row4 col0\" >HNL</td>\n",
    "      <td id=\"T_fc661_row4_col1\" class=\"data row4 col1\" >OGG</td>\n",
    "      <td id=\"T_fc661_row4_col2\" class=\"data row4 col2\" >14043.8</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td id=\"T_fc661_row5_col0\" class=\"data row5 col0\" >BOS</td>\n",
    "      <td id=\"T_fc661_row5_col1\" class=\"data row5 col1\" >LGA</td>\n",
    "      <td id=\"T_fc661_row5_col2\" class=\"data row5 col2\" >13865.1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td id=\"T_fc661_row6_col0\" class=\"data row6 col0\" >LGA</td>\n",
    "      <td id=\"T_fc661_row6_col1\" class=\"data row6 col1\" >BOS</td>\n",
    "      <td id=\"T_fc661_row6_col2\" class=\"data row6 col2\" >13674.9</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td id=\"T_fc661_row7_col0\" class=\"data row7 col0\" >OGG</td>\n",
    "      <td id=\"T_fc661_row7_col1\" class=\"data row7 col1\" >HNL</td>\n",
    "      <td id=\"T_fc661_row7_col2\" class=\"data row7 col2\" >13490.9</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td id=\"T_fc661_row8_col0\" class=\"data row8 col0\" >OAK</td>\n",
    "      <td id=\"T_fc661_row8_col1\" class=\"data row8 col1\" >JFK</td>\n",
    "      <td id=\"T_fc661_row8_col2\" class=\"data row8 col2\" >12615.6</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td id=\"T_fc661_row9_col0\" class=\"data row9 col0\" >HOU</td>\n",
    "      <td id=\"T_fc661_row9_col1\" class=\"data row9 col1\" >MSY</td>\n",
    "      <td id=\"T_fc661_row9_col2\" class=\"data row9 col2\" >11942.8</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "> These routes are characterized by high passenger traffic, indicating strong demand, such as *Dallas to Houston* and *Houston to Dallas* that the ones with the highest average.\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Nodes' Contribution (Q2)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implement a function `analyze_centrality(flight_network, airport)` that computes the following centrality measures for a given airport:\n",
    "\n",
    "\n",
    "    - *Betweenness* *centrality*: Measures how often a node appears on the shortest paths between other nodes.\n",
    "    \n",
    "    - *Closeness* *centrality*: Measures how easily a node can access all other nodes in the network.\n",
    "    \n",
    "    - *Degree* *centrality*: Simply counts the number of direct connections to the node.\n",
    "    \n",
    "    - *PageRank*: Computes the \"importance\" of a node based on incoming connections and their weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_centrality(flight_network, airport):\n",
    "#Computes centrality measures for a given airport in the flight network.\n",
    "   # Degree Centrality (normalized by default)\n",
    "    degree_centrality = nx.degree_centrality(flight_network)\n",
    "    # Closeness Centrality\n",
    "    closeness_centrality = nx.closeness_centrality(flight_network)\n",
    "    # Betweenness Centrality\n",
    "    betweenness_centrality = nx.betweenness_centrality(flight_network, weight='weight')\n",
    "    # PageRank (weighted by 'weight')\n",
    "    pagerank = nx.pagerank(flight_network, weight='weight')\n",
    "    # Results for the specific airport\n",
    "    centrality_measures = {\n",
    "        'Degree Centrality': degree_centrality.get(airport, 0),\n",
    "        'Closeness Centrality': closeness_centrality.get(airport, 0),\n",
    "        'Betweenness Centrality': betweenness_centrality.get(airport, 0),\n",
    "        'PageRank': pagerank.get(airport, 0)\n",
    "    }\n",
    "    return centrality_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_centrality(graph,\"OGG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Write a function `compare_centralities(flight_network)` to:\n",
    "\n",
    "    - Compute and compare centrality values for all nodes in the graph.\n",
    "    \n",
    "    - Plot centrality distributions (histograms for each centrality measure).\n",
    "    \n",
    "    - Return the top 5 airports for each centrality measure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_centralities(flight_network):\n",
    "    #centrality measures using networkx\n",
    "    degree_centrality = nx.degree_centrality(flight_network)\n",
    "    closeness_centrality = nx.closeness_centrality(flight_network)\n",
    "    betweenness_centrality = nx.betweenness_centrality(flight_network, weight='weight')\n",
    "    pagerank = nx.pagerank(flight_network, weight='weight')\n",
    "\n",
    "    #comparison dataframe\n",
    "    centrality_df = pd.DataFrame({\n",
    "        'Airport': list(degree_centrality.keys()),\n",
    "        'Degree Centrality': list(degree_centrality.values()),\n",
    "        'Closeness Centrality': [closeness_centrality[node] for node in degree_centrality],\n",
    "        'Betweenness Centrality': [betweenness_centrality[node] for node in degree_centrality],\n",
    "        'PageRank': [pagerank[node] for node in degree_centrality]\n",
    "    })\n",
    "\n",
    "    # Plot histograms \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(\"Centrality Distributions for Airports\", fontsize=16)\n",
    "\n",
    "    centrality_df['Degree Centrality'].hist(ax=axes[0, 0], bins=50, color='skyblue')\n",
    "    axes[0, 0].set_title(\"Degree Centrality Distribution\")\n",
    "    axes[0, 0].set_xlabel(\"Degree Centrality\")\n",
    "    axes[0, 0].set_ylabel(\"Frequency\")\n",
    "\n",
    "    centrality_df['Closeness Centrality'].hist(ax=axes[0, 1], bins=50, color='lightgreen')\n",
    "    axes[0, 1].set_title(\"Closeness Centrality Distribution\")\n",
    "    axes[0, 1].set_xlabel(\"Closeness Centrality\")\n",
    "    axes[0, 1].set_ylabel(\"Frequency\")\n",
    "\n",
    "    centrality_df['Betweenness Centrality'].hist(ax=axes[1, 0], bins=50, color='lightcoral')\n",
    "    axes[1, 0].set_title(\"Betweenness Centrality Distribution\")\n",
    "    axes[1, 0].set_xlabel(\"Betweenness Centrality\")\n",
    "    axes[1, 0].set_ylabel(\"Frequency\")\n",
    "\n",
    "    centrality_df['PageRank'].hist(ax=axes[1, 1], bins=50, color='gold')\n",
    "    axes[1, 1].set_title(\"PageRank Distribution\")\n",
    "    axes[1, 1].set_xlabel(\"PageRank\")\n",
    "    axes[1, 1].set_ylabel(\"Frequency\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "    #top 5 airports\n",
    "    top_airports = {\n",
    "        'Top Degree Centrality': centrality_df.nlargest(5, 'Degree Centrality')[['Airport', 'Degree Centrality']],\n",
    "        'Top Closeness Centrality': centrality_df.nlargest(5, 'Closeness Centrality')[['Airport', 'Closeness Centrality']],\n",
    "        'Top Betweenness Centrality': centrality_df.nlargest(5, 'Betweenness Centrality')[['Airport', 'Betweenness Centrality']],\n",
    "        'Top PageRank': centrality_df.nlargest(5, 'PageRank')[['Airport', 'PageRank']]\n",
    "    }\n",
    "\n",
    "    return top_airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_centralities(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Ask LLM (eg. ChatGPT) to suggest alternative centrality measures that might be relevant to this task. How can you check that the results given by the LLM are trustable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here is the output of ChatGpt:\n",
    "| **Centrality Measure**         | **Relevance**                                                | **When to Use**                          |\n",
    "|--------------------------------|-------------------------------------------------------------|------------------------------------------|\n",
    "| **Eigenvector Centrality**     | Importance based on connections to influential nodes.       | Identify hubs connected to other hubs.   |\n",
    "| **Katz Centrality**            | Balances influence of hubs and smaller nodes.               | Highlight strategic regional airports.   |\n",
    "| **Harmonic Centrality**        | Accounts for disconnected nodes.                            | Sparse or disconnected networks.         |\n",
    "| **Load Centrality**            | Measures flow through nodes.                                | Critical intermediaries for traffic.     |\n",
    "| **Clustering Coefficient**     | Tendency to form local, tight clusters.                     | Identify regional clusters of routes.    |\n",
    "| **Edge Betweenness Centrality**| Identifies critical edges in the network.                   | Find key flight routes.                  |\n",
    "| **Strength Centrality**        | Weighted degree: accounts for passenger/traffic volume.     | Identify traffic-heavy airports.         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to check his reliability we can check with a correlation methods to compare the centrality scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Implement one of these measures suggested by the LLM, compare its results to the centralities you've already computed, and analyze whether it adds any new insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_alternative_centralities(flight_network):\n",
    "    \"\"\"\n",
    "    Calculate Degree Centrality and Eigenvector Centrality for a given graph.\n",
    "    Returns a DataFrame with centrality measures.\n",
    "    \"\"\"\n",
    "    # Degree Centrality\n",
    "    degree_centrality = nx.degree_centrality(flight_network)\n",
    "\n",
    "    # Eigenvector Centrality\n",
    "    eigenvector_centrality = nx.eigenvector_centrality(flight_network, weight='weight')\n",
    "\n",
    "    # Create a DataFrame with centrality measures\n",
    "    centrality_df2 = pd.DataFrame({\n",
    "        'Airport': list(degree_centrality.keys()),\n",
    "        'Degree Centrality': list(degree_centrality.values()),\n",
    "        'Eigenvector Centrality': [eigenvector_centrality[node] for node in degree_centrality]\n",
    "    })\n",
    "\n",
    "    return centrality_df2\n",
    "\n",
    "def plot_centrality_distributions(centrality_df):\n",
    "    \"\"\"\n",
    "    Plot the distributions of Degree Centrality and Eigenvector Centrality.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Degree Centrality Histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(centrality_df['Degree Centrality'], bins=30, color='skyblue')\n",
    "    plt.title('Degree Centrality Distribution')\n",
    "    plt.xlabel('Degree Centrality')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Eigenvector Centrality Histogram\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(centrality_df['Eigenvector Centrality'], bins=30, color='lightcoral')\n",
    "    plt.title('Eigenvector Centrality Distribution')\n",
    "    plt.xlabel('Eigenvector Centrality')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_df2=calculate_alternative_centralities(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_centrality_correlation(centrality_df):\n",
    "    degree = centrality_df['Degree Centrality']\n",
    "    eigenvector = centrality_df['Eigenvector Centrality']\n",
    "\n",
    "    #Spearman's rank correlation\n",
    "    correlation, p_value = spearmanr(degree, eigenvector)\n",
    "\n",
    "    print(\"Spearman's Rank Correlation between Degree and Eigenvector Centrality:\")\n",
    "    print(f\"Correlation Coefficient: {correlation:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4e}\")\n",
    "\n",
    "    return correlation, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_centrality_correlation(centrality_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "because the correlation coefficient its high (its maximum its 1) we can consider his output trustable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## **Finding Best Routes (Q3)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this task, you need to implement a function that, given an origin and destination city, determines the best possible route between them. To simplify, the focus will be limited to flights operating on a specific day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Each city may have multiple airports; in such cases, the function should calculate the best route for every possible airport pair between the two cities. For example, if city A has airports $a_1$ , $a_2$ and city B has $b_1$ , $b_2$ , the function should compute the best routes for $a_1 → b_1$ , $a_1 → b_2$ , $a_2 → b_1$ and $a_2 → b_2$ . If it’s not possible to travel from one airport in the origin city to another airport in the destination city on that date, you must report it as well.\n",
    "\n",
    "The function takes the following inputs:\n",
    "\n",
    "1. Flights network\n",
    "\n",
    "2. Origin city name\n",
    "\n",
    "3. Destination city name\n",
    "\n",
    "4. Considered Date (in yyyy-mm-dd format)\n",
    "\n",
    "The function output:\n",
    "\n",
    "1. A table with three columns: 'Origin_city_airport', 'Destination_city_airport', and the 'Best_route'.\n",
    "\n",
    "**Note**: In the \"Best_route\" column, we expect a list of airport names connected by → , showing the order in which they are to be visited during the optimal route. If no such route exists, the entry should display \"No route found.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We choose the date with the most flights in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Date = df.groupby('Fly_date')['Origin_airport'].count().idxmax()\n",
    "Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After debug and analyzing our `distances_dictionary` from *Seattle*, we choose one of the routes with the highest `best_distance` in order to highlight the sequence of flights that brings to the destination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Origin_city = \"Seattle\"\n",
    "Destination_city = \"Bloomington\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To compute the best route we can use the **Dijkstra's algorithm**.\n",
    "\n",
    "#### **Dijkstra's algorithm**\n",
    "\n",
    "\n",
    "Here it is a pseudocode:\n",
    "\n",
    "```python\n",
    "\n",
    "1. Initialize a priority queue (min-heap) called 'pq'\n",
    "\n",
    "2. For each node in the graph:\n",
    "\n",
    "    a. Set the distance to the source node as 0\n",
    "\n",
    "    b. Set the distance to all other nodes as infinity\n",
    "\n",
    "    c. Set the previous node for all nodes as undefined (None)\n",
    "\n",
    "3. Insert the source node into the priority queue with distance 0\n",
    "\n",
    "4. While the priority queue 'pq' is not empty:\n",
    "\n",
    "    - Pop the node 'current' with the smallest distance from 'pq'\n",
    "\n",
    "    - For each neighbor of the current node:\n",
    "\n",
    "        - Calculate the new distance to the neighbor: new_distance = current_distance + edge_weight\n",
    "        \n",
    "        - If the new distance is smaller than the current known distance to the neighbor:\n",
    "\n",
    "            - Update the shortest distance to this neighbor\n",
    "\n",
    "            - Set the previous node to the current node\n",
    "\n",
    "            - Insert the neighbor into the priority queue with the updated distance\n",
    "\n",
    "5. After all nodes are processed, the algorithm ends.\n",
    "\n",
    "6. The shortest path from the source to any node can be reconstructed by following the previous nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function** `compute_best_route(graph, origin, destination, date)`\n",
    "\n",
    "The Function `compute_best_route` computes the best route in a specific date between two cities.\n",
    "\n",
    "- The graph is filtered using `filter_graph_by_date` function that creates a new graph adding only the edges relative to the input date\n",
    "\n",
    "- Then we iterate over the all the pairs between the airports in the source and destination cities and minimize the distance between them using the `compute_Dijkstra` function. Since the Dijkstra algorithm computes a dictionary of distances from the minimized sources, we can apply it only once and then iterate over the number of airports in the destination city indexing in the `distances_dict`\n",
    "\n",
    "- If the distance computed is less than the best_distance so update it and apply the `reconstruct_path` function where it builds the path to reach the destination iterating over the `prev` dictionary\n",
    "\n",
    "- We return a Dataframe with all the infos needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = functions.compute_best_route(graph, Origin_city, Destination_city, Date)\n",
    "\n",
    "# To hide the index of the row (only row of the dataframe)\n",
    "table.style.hide(axis=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Airline Network Partitioning (Q4)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In graph theory, this task is known as a graph disconnection problem. Your goal is to write a function that removes the minimum number of flights between airports to separate the original flight network into two disconnected subgraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Finding and Extracting Communities (Q5)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In this task, you are asked to analyze the graph and identify the communities based on the flight network provided. For the airline, the primary focus is on the cities, so your communities should reflect the connectivity between cities through the flights that link them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based ong the function created in Q1:\n",
    "\n",
    "**Function** `create_airport_graph`\n",
    "\n",
    "Our goal is to analyze communities through a function that takes in the flight network and two cities name, and outputs the total number of communities, has to visualize the graph highlighting the communities with different colors and tell if the two cities are within the same commnunity.\n",
    "\n",
    "To do this we are going to use Louvain algorithm, which is more suitable for a complex graph, like the one we are analyzing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: `modularity`\n",
    "\n",
    "This function calculates the **modularity** of a partition in a graph, which is a measure of the strength of division of a network into communities. A higher modularity score indicates a stronger division of the network into well-defined communities. The function works for undirected graphs and takes into account the edge weights.\n",
    "\n",
    "#### Parameters:\n",
    "- **G** (`networkx.Graph`): An object representing the network.\n",
    "  - The weights of the edges represent the strength of the connection between nodes, in this case the distance between the airports\n",
    "  \n",
    "- **partition** (`dict`): A dictionary where:\n",
    "  - **Key**: Node (representation of an airport).\n",
    "  - **Value**: The community number that the node belongs to.\n",
    "\n",
    "#### Returns:\n",
    "- **float**: The modularity score of the given partition. A higher modularity score indicates better community structure.\n",
    "\n",
    "#### Algorithm:\n",
    "1. **Degree Calculation**: The degree of each node is computed based on the weights of the edges.\n",
    "2. **Community Structure**: The nodes are grouped into their respective communities as per the `partition` dictionary.\n",
    "3. **Modularity Formula**:\n",
    "\n",
    "  $$ Q = \\frac{1}{2m} \\sum_{c} \\left( \\sum_{i,j \\in c} A_{ij} - \\frac{d_i d_j}{2m} \\right) $$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;where:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;- $A_{ij}$ is the edge weight between nodes $i$ and $j$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;- $d_i, d_j$ are degrees of nodes $i$ and $j$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;- $m$ is total edge weight in the graph  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;- sum over all node pairs in community $c$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modularity(G, partition):\n",
    "    m = G.size(weight='weight')\n",
    "    degrees = dict(G.degree(weight='weight'))\n",
    "    communities = {}\n",
    "    \n",
    "    for node, community in partition.items():\n",
    "        if community not in communities:\n",
    "            communities[community] = []\n",
    "        communities[community].append(node)\n",
    "    \n",
    "    q = 0\n",
    "    for community, nodes in communities.items():\n",
    "        l_c = sum([G[u][v]['weight'] for u in nodes for v in nodes if G.has_edge(u, v)])\n",
    "        d_c = sum([degrees[node] for node in nodes])\n",
    "        q += l_c / (2 * m) - (d_c / (2 * m))**2\n",
    "    \n",
    "    return q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: `louvain_method`\n",
    "\n",
    "It is an algorithm that finds communities optimizing modularity.\n",
    "\n",
    "1. **How does it work?**\n",
    "   - Iterates through each node\n",
    "   - Tests moving node to neighboring communities\n",
    "   - Calculates modularity change for each potential move  \n",
    "   - Selects move giving highest modularity increase\n",
    "   - Repeats until no moves improve modularity\n",
    "\n",
    "2. **Implementation**\n",
    "   - Initial state: each node in separate community\n",
    "   - `move_node()`: handles single iteration of node movements\n",
    "   - Main loop continues until no further improvements possible\n",
    "   - Returns final community assignments as dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def louvain_method(G):\n",
    "    def move_node(G, partition):\n",
    "        moved = False\n",
    "        for node in G.nodes():\n",
    "            best_community = partition[node]\n",
    "            best_increase = 0\n",
    "            for neighbor in G.neighbors(node):\n",
    "                community = partition[neighbor]\n",
    "                if community == partition[node]:\n",
    "                    continue\n",
    "                original_modularity = modularity(G, partition)\n",
    "                partition[node] = community\n",
    "                new_modularity = modularity(G, partition)\n",
    "                increase = new_modularity - original_modularity\n",
    "                if increase > best_increase:\n",
    "                    best_community = community\n",
    "                    best_increase = increase\n",
    "                partition[node] = best_community\n",
    "            if best_community != partition[node]:\n",
    "                partition[node] = best_community\n",
    "                moved = True\n",
    "        return moved\n",
    "\n",
    "    partition = {node: i for i, node in enumerate(G.nodes())}\n",
    "    while True:\n",
    "        if not move_node(G, partition):\n",
    "            break\n",
    "\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `analyze_flight_network_louvain_fs`\n",
    "\n",
    "Main function to apply the algorithm.\n",
    "\n",
    "**Steps**\n",
    "1. Community detection\n",
    "2. Checking if the two cities given as input are part of the same community\n",
    "3. Creates a Folium map to represent the communities, dividing them by color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "def analyze_flight_network_louvain_fs(G, city1, city2):\n",
    "\n",
    "    G_undirected = G.to_undirected()\n",
    "\n",
    "    partition = louvain_method(G_undirected)\n",
    "    \n",
    "    num_communities = len(set(partition.values()))\n",
    "\n",
    "    communities = {}\n",
    "    for node, community in partition.items():\n",
    "        communities.setdefault(community, []).append(node)\n",
    "    \n",
    "    city1_community = None\n",
    "    city2_community = None\n",
    "    \n",
    "    for community, nodes in communities.items():\n",
    "        community_cities = {G.nodes[node]['city'] for node in nodes}\n",
    "        if city1 in community_cities:\n",
    "            city1_community = community\n",
    "        if city2 in community_cities:\n",
    "            city2_community = community\n",
    "\n",
    "    same_community = (city1_community == city2_community)\n",
    "\n",
    "    map_center = [df['Org_airport_lat'].mean(), df['Org_airport_long'].mean()]\n",
    "    m = folium.Map(location=map_center, zoom_start=5)\n",
    "\n",
    "    np.random.seed(0)\n",
    "    colors = np.random.randint(0, 256, size=(num_communities, 3))\n",
    "    colors = [f'#{r:02x}{g:02x}{b:02x}' for r, g, b in colors]\n",
    "\n",
    "    for community, nodes in communities.items():\n",
    "        for node in nodes:\n",
    "            lat = G.nodes[node]['lat']\n",
    "            long = G.nodes[node]['long']\n",
    "            city = G.nodes[node]['city']\n",
    "            folium.CircleMarker(\n",
    "                location=[lat, long],\n",
    "                radius=5,\n",
    "                color=colors[community % len(colors)],\n",
    "                fill=True,\n",
    "                fill_color=colors[community % len(colors)],\n",
    "                fill_opacity=0.6,\n",
    "                popup=city\n",
    "            ).add_to(m)\n",
    "\n",
    "    '''\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        origin_lat = G.nodes[u]['lat']\n",
    "        origin_long = G.nodes[u]['long']\n",
    "        dest_lat = G.nodes[v]['lat']\n",
    "        dest_long = G.nodes[v]['long']\n",
    "        folium.PolyLine(\n",
    "            locations=[(origin_lat, origin_long), (dest_lat, dest_long)],\n",
    "            color='gray',\n",
    "            weight=1,\n",
    "            opacity=0.5\n",
    "        ).add_to(m)'''\n",
    "\n",
    "    return num_communities, communities, same_community, m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_communities, communities, same_community, map = analyze_flight_network_louvain_fs(graph.copy(), 'Sacramento, CA', 'Los Angeles. CA')\n",
    "print(f\"Number of communities: {num_communities}\")\n",
    "print(f\"Communities: {communities}\")\n",
    "print(f\"Do City1 and City2 belong to the same community? {same_community}\")\n",
    "map.save(\"flight_network_map.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Ask a LLM (ChatGPT, Claude AI, Gemini, Perplexity, etc.) to suggest an alternative algorithm for extracting communities and explain the steps required to implement it. Then, implement this algorithm and compare its results with the current method you've chosen. Discuss the differences in the outcomes and analyze which approach you think is better, providing reasons for your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLM suggestion**\n",
    "\n",
    "An alternative method, suggested by ChatGPT, for community detection is the **Label Propagation Algorithm**. \n",
    "\n",
    "This algorithm works with the following procedure:\n",
    "1. Initialization: assigning a unique label to each node in the network --> This step uses the function created in QA1 to create the graph\n",
    "2. Label Propagation: the label of each node gets updated based on the most frequent label among its neighbors.\n",
    "3. Convergence: label propagation is repeated unitl the label no longer change or a maximum number of iterations is reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Propagation Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "def label_prop(Graph, max_iter=100, tol=0.1):\n",
    "    nodes = list(Graph.nodes())\n",
    "    labels = {node: i for i, node in enumerate(nodes)}\n",
    "    \n",
    "    iter_count = 0\n",
    "    stabilized = False\n",
    "    \n",
    "    while not stabilized and iter_count < max_iter:\n",
    "        iter_count += 1\n",
    "        old_labels = labels.copy()\n",
    "        random.shuffle(nodes)\n",
    "        \n",
    "        for node in nodes:\n",
    "            if not list(Graph.neighbors(node)):\n",
    "                continue\n",
    "                \n",
    "            # Weight labels by edge weights if available\n",
    "            neighbor_labels = []\n",
    "            for neighbor in Graph.neighbors(node):\n",
    "                weight = Graph[node][neighbor].get('weight', 1.0)\n",
    "                neighbor_labels.extend([labels[neighbor]] * int(weight * 10))\n",
    "            \n",
    "            if neighbor_labels:\n",
    "                # Get top 3 most common labels\n",
    "                most_common = Counter(neighbor_labels).most_common(1)\n",
    "                # Randomly select from top labels with preference to more common ones\n",
    "                weights = [count for _, count in most_common]\n",
    "                new_label = random.choices([label for label, _ in most_common], \n",
    "                                        weights=weights)[0]\n",
    "                labels[node] = new_label\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis using LPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_flight_network_lpa(Graph, city1, city2):\n",
    "\n",
    "    G_undirected = Graph.to_undirected()\n",
    "\n",
    "    for u, v in G_undirected.edges():\n",
    "        lat1, lon1 = G_undirected.nodes[u]['lat'], G_undirected.nodes[u]['long']\n",
    "        lat2, lon2 = G_undirected.nodes[v]['lat'], G_undirected.nodes[v]['long']\n",
    "        dist = np.sqrt((lat1 - lat2)**2 + (lon1 - lon2)**2)\n",
    "        G_undirected[u][v]['weight'] = 1 / (1 + dist)\n",
    "\n",
    "    partition = label_prop(G_undirected)\n",
    "\n",
    "    num_communities = len(set(partition.values()))\n",
    "\n",
    "    communities = {}\n",
    "    for node, community in partition.items():\n",
    "        if community not in communities:\n",
    "            communities[community] = []\n",
    "        communities[community].append(node)\n",
    "\n",
    "    city1_community = None\n",
    "    city2_community = None\n",
    "\n",
    "    for community, nodes in communities.items():\n",
    "        community_cities = {Graph.nodes[node]['city'] for node in nodes}\n",
    "        if city1 in community_cities:\n",
    "            city1_community = community\n",
    "        if city2 in community_cities:\n",
    "            city2_community = community\n",
    "    \n",
    "    same_community = (city1_community == city2_community)\n",
    "\n",
    "    map_center = [\n",
    "        np.mean([Graph.nodes[node]['lat'] for node in Graph.nodes()]),\n",
    "        np.mean([Graph.nodes[node]['long'] for node in Graph.nodes()])\n",
    "    ]\n",
    "    map = folium.Map(location=map_center, zoom_start=5)\n",
    "\n",
    "    np.random.seed(0)\n",
    "    colors = np.random.randint(0, 256, size=(num_communities, 3))\n",
    "    colors = [f'#{r:02x}{g:02x}{b:02x}' for r, g, b in colors]\n",
    "\n",
    "    '''\n",
    "    for (u, v) in G_undirected.edges():\n",
    "        coords = [(Graph.nodes[u]['lat'], Graph.nodes[u]['long']),\n",
    "                 (Graph.nodes[v]['lat'], Graph.nodes[v]['long'])]\n",
    "        folium.PolyLine(coords, weight=1, color='blue', opacity=0.1).add_to(map)'''\n",
    "\n",
    "    for community, nodes in communities.items():\n",
    "        for node in nodes:\n",
    "            lat = Graph.nodes[node]['lat']\n",
    "            long = Graph.nodes[node]['long']\n",
    "            city = Graph.nodes[node]['city']\n",
    "            folium.CircleMarker(\n",
    "                location=[lat, long],\n",
    "                radius=5,\n",
    "                color=colors[community % len(colors)],\n",
    "                fill_opacity=0.6,\n",
    "                popup=city\n",
    "            ).add_to(map)\n",
    "    \n",
    "    return num_communities, communities, same_community, map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_communities, communities, same_community, map_lpa = analyze_flight_network_lpa(graph.copy(), 'Sacramento, CA', 'Chicago, IL')\n",
    "print(f\"Number of communities: {num_communities}\")\n",
    "print(f\"Communities: {communities}\")\n",
    "print(f\"Do City1 and City2 belong to the same community? {same_community}\")\n",
    "\n",
    "map_lpa.save(\"flight_network_map_lpa.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Result comparison**\n",
    "It is possible to see differences in the number of communities the two algorithms are able to identify; **Louvain algorithm** was able to identify 38 communities, while **Label Propagation** just 17.\n",
    "Time complexity is sensibly lower in Label Propagation Algorithm.\n",
    "<br><br>\n",
    "Visually, the map generated using the **Label Propagation Algorithm** seems to identify communities close to each other; while Louvain seems able to identify communities where nodes are not close to each other.\n",
    "<br><br>\n",
    "In my opinion Louvain is better because of its ability to identify connections where nodes are not actually close to each other, that might exist in dividing the graph, based on how the edges are formed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bonus Question - Connected Components on MapReduce**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In this task, you are required to use PySpark and the MapReduce paradigm to identify the connected components in a flight network graph. The focus should be on airports rather than cities. As you know, a connected component refers to a group of airports where every pair of airports within the group is connected either directly or indirectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compare the execution time and the results of your implementation with those of the GraphFrames package for identifying connected components. If there is any difference in the results, provide an explanation for why that might occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Algorithmic Question (AQ)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arya needs to travel between cities using a network of flights. Each flight has a fixed cost (in euros), and she wants to find the cheapest possible way to travel from her starting city to her destination city. However, there are some constraints on the journey:\n",
    "\n",
    "- Arya can make at most `k` stops during her trip (this means up to `k+1` flights).\n",
    "    \n",
    "- If no valid route exists within these constraints, the result should be `-1`.\n",
    "\n",
    "Given a graph of cities connected by flights, your job is to find the minimum cost for Arya to travel between two specified cities (`src` to `dst`) while following the constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Write a pseudocode that describes the algorithm to find the cheapest route with at most k stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Implement the algorithm in Python and simulate the given test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Analyze the algorithm's efficiency. Provide its time complexity and space complexity, and explain whether it is efficient for large graphs (e.g., `n > 100`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Optimize the algorithm to handle larger graphs. Provide an updated pseudocode and analyze the computational complexity of your optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** Ask LLM (e.g., ChatGPT) for an optimized version of your algorithm. Compare its solution to yours in terms of performance, time complexity, and correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
